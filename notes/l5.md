# High Dimension Similarity Search

## Similarity Search

* Problem Definition:
  $$ \text{Given a query } q \text{ and data set } D, \text{ find } o \in D, \text{ where } o \text{ is similar to } q

* Two types of similarity search:
  1. Range Search: $dist(o, q) \le \tau$
  2. Nearest neighbor search: $dist(o^*, q) \le dist(o, q), \forall o \in D$
	
		> Top-k version is simply get the *k-nearest* results

> Sometimes, two similarity searches can be converted into each other
> * [RS] to [NNS]: decrement for $\tau$ until the number of results hits the threshold
> * [NNS] to [RS]: increment for $k$ until the next $dist(o^*, q) \ge dist(o, q)$

* Distance/similarity function varies: Euclidean/Jaccard/Inner Product

### Similarity Search in 1-D

Data in 1-D are just numbers, generally can be solved with binary search, binary search tree and B+ Tree.

> Presumption behind all these algorithms/data structures is that data can be sorted.
> The time complexity drops from $O(n)$ to $O(\log n)$.
> Which (order/sort) is hard to be implemented in higher dimension.

### Similarity Search in 2-D

1. Partition based algorithms
    * Partition data into "cells"
    * Nearest neighbors are in the same cell with query or *adjacent cells*

		> There will be uncertainties because of the partition method users just defined. The data point in the same cell as the query is not necessarily the nearest neighbor to the query. There might be another data point more close to the query but being partitioned into another partition.

		* The number of "cells" to probe on $n$-dimensional space is $3^n$

		> * 1-D: data forms a line, there will only be three points matters (left, middle, right).
		> * 2-D: data forms a square, there will be nine cells matters (N, S, E, W, NE, NW, SE, SE, center).
		> * 3-D: data forms a cube, there will be $9 \times 3$ small cubes matters.
		

### Similarity Search in Metric Space

> Wikipedia: Metric space (think Euclidean Space)</br>
> In mathematics, a metric space is a set together with a metric on the set. The metric is a function defines a concept of distance between any two members of the set, which are usually called points.

* Triangle inequality
	$$ dist(x, q) \le dist(x, y) + dist(y, q) $$
* Orchard's Algorithm

	```python
	p_list = []
	for x in D:
		p_list.append(sorted(d()))	
	```

### Curse of Dimensionality

Generally speaking, the curse of dimensionality refers to phenomena that occur when classifying, organizing and analyzing high dimensional data that does not occur in low dimensional spaces, specially the issue of data sparsity and "closeness" of data.

When moving to the high dimension, the volume of the space represented grows so quickly (exponentially) that the data cannot keep up and thus becomes sparse. As a result, the given data fills less and less of the metric space. In order to keep the pace of space growing, the data for analysis also needs to grows exponentially.

The second issue is relating to sorting or classifying the data. In low dimensional spaces, data may seems similar (as there are limited representations/meanings in lower dimension) while the high the dimension the further these data points may seem to be.

[More information about The Curse of Dimensionality](https://deepai.org/machine-learning-glossary-and-terms/curse-of-dimensionality)

### Similarity Search in High Dimension Space

* Approximate Nearest Neighbor Search (ANNS)
	* Reason: There is no sub-linear solution to find the exact result of a nearest neighbor query. So that the condition needs to be relaxed.
	* Success: As the algorithm evaluates the nearest neighbor in a approximate way. There is a success rate for each operation. In other words, the algorithm does not guarantee to find the true nearest neighbor every time.
		* *c-approximate NN Search*: returns $o$ such that
		$$ dist(o, q) \le c \cdot dist(o^*, q) $$
		* Bound Success Rate with Locality Sensitive Hashing (LSH)

## Locality Sensitive Hashing

### Intuition

1. Set thresholds/bounds
	* Index: Instead of using tradition hashing functions that generate total distinct hash keys for two slightly different values, we want to modify the hashing function to be more error tolerant. Thus, the modified hashing functions create same hash key for the similar data (i.e., no longer needs to be the exactly same).

	* Retrieval:
		1. Compute the hash key for the query
		2. Obtain all the data has the same key with query (i.e., candidate generation)
		3. Find the nearest one to the query (i.e., verification)

		* Analysis:
			* Space: $O(n)$
			* Time: $O(1) + O(|candidate|)$ (i.e., look up hash table and verify candidates)
			* Better than linear search

2. LSH Functions:
  * Given point $o_1, o_2$, distance $r_1, r_2$, probability $p_1, p_2$
		> $r_1$ restrains similar while $r_2$ restrains dissimilar
  * An LSh function $h(\cdot)$ should satisfy:
    * $Pr[h(o_1) = h(o_2)] \ge p_1, \text{ if } dist(o_1, o_2) \le r_1$

		> saying if $o_1$ and $o_2$ are similar (in range of $r_1$), then the probability of they sharing the same hash key should be higher than the threshold

    * $Pr[h(o_1) = h(o_2)] \le p_2, \text{ if } dist(o_1, o_2) > r_2$

		> saying if $o_1$ and $o_2$ are dissimilar (in range of $r_2$), then the probability of they sharing the same hash key should be lower than the threshold

### LSH functions

1. MinHash for Jaccard Similarity

* Each data object is considered to be a **set**
* $Jaccard(S_1, S_2) = \dfrac{\big|S_i \cap S_j\big|}{\big|S_i \cup S_j\big|}$
* Mathematical Interpretation: Two sets are more similar (i.e., have relatively more members in common) when their Jaccard index is closer to 1. The Goal of MinHash is to estimate $J(S_1, S_2)$ quickly, without explicitly computing the intersection and union.
* Proof: The basic idea is that assuming there are no hashing collisions, $e in S_1 \cap S_2$ if and only if $h(S_1) = h(S_2) = e$, where $e$ is the smallest member (i.e., index[0]) of some random permutation.

2. SimHash - LSH function for Angular Distance

* Each data object is considered to be a **d-dimensional vector**
* Mathematical Interpretation: Let $h(x; a) = sign(a^Tx)$. As long as x and y are having the same sign (i.e., they are on the same side of the hyperplane), they are determined as similar objects and sharing the same hashing key.
* Proof: Object $o_1$ and $o_2$ is going to form two hyperplanes in the metric space. We only care the angular distance between two hyperplanes. We start by randomly generating a normal vector $\textbf{a}$ following standard distribution. Supposing the angle in between is $\theta$, then the probability of that $o_1$ and $o_2$ don't share the same hashing key is the probability the hyperplane normal vector $\textbf{a}$ hits the space between $o_1$ and $o_2$ (i.e., $\frac{\theta}{\pi}$). Or $Pr[h(o_1) = h(o_2)] = 1 - \dfrac{\theta}{\pi}$.

3. p-stable LSH - LSH function for Euclidean

* Each data object is considered to be a **d-dimensional vector** (i.e., Euclidean distance can be later applied)
* Mathematical Interpretation: Each data objects would be projected into the normal vector. Then apply a specific weight to the normal vector and split the vector into a series equal length trunks. The data objects that are projected in the same trunks are more likely to have the same hashing key.
* Problems: The problem of single hash function is it is hard to distinguish if two pairs are close to each other. The ideal relations between the similarity and the probability are each though the distances between two objects are small, they are still able to be distinguished.

### AND-OR Composition for multiple hash function

* Combine $k$ hashes together, using AND operation
  * One must match all the hashes
  * $Pr[H_{AND}(o_1) = H_{AND}(o_2)] = {p_{o_1, o_2}}^k$
	* Problem: Probability for similar objects is not large enough
* Combine $l$ hashes together, using OR operation
  * One need to match at least one of the hashes
  * $Pr[H_{OR}(o_1) = H_{OR}(o_2)] = 1 - (1 - p_{o_1, o_2})^l$
	* Problem: Probability for dissimilar objects is not small enough
* Combine using both of AND and OR operations
	1. Let $h_{i,j}$ be LSH functions, $i \in \{1,2,\ldots,l\}, j \in \{1,2,\ldots,k\}$
	2. AND
		* First combine $k$ hashes using AND operation
		$$ H_i(o) = [h_{i,1}(o),h_{i,2}(o),\ldots,h_{i,k}(o)] $$
		satisfy the condition
		$$ H_i(o_1) = H_i(o_2) \iff \forall j \in \{1,2,\ldots,k\}, h_{i,j}(o_1) = h_{i,j}(o_2) $$
	3. OR
		* Data point $o$ is a nearest neighbor candidate of $q$ if
		$$ \exist i \in \{1,2,\ldots,l\}, H_i(o) = H_i(q) $$
		* The probability of $o$ is a candidate of $q$ is
		$$ Pr[H(o)=H(q)] = 1-(1-p_{q,o}^{k})^l $$
> Note there are tradeoffs between space and space, and effectiveness, while normally, the large $l$ and $k$ result in a effective LSH (higher probability/lower probability bound or say less smooth). But the large $l$ and $k$ will consume much more space to actually store the hash functions.

### False Positives and False Negatives

* False Positive: **returned data with dissimilar objects** (i.e., $dist(o, q) > r_2$)
	> False positive refers to the data that is not supposed to be marked as positive, as they actually should be marked as negative
* False Negative: **not returned data with similar objects** (i.e., $dist(o, q) < r_1$)
	> False negative refers to the data that is not supposed to be marked as negative, as they actually should be marked as positive

### Conclusion

The framework of NNS using LSH is basically,

1. Pre-processing to generate LSh functions (minHash/simHash/p-stable).
2. Index superhash for object $o$ in a range of $i \in \{1,\ldots,l\}$.
	> Hash table stores {'key': value, 'superhash/$H_i(o)$': id of object $o$}
3. Dealing with queries
   1. Compute superhash for query $q$, denoted $H_i(q)$
	 2. Generate candidate set satisfying there at least one hashing key are equal, denoted 
	 $$\exist i \in \{1,2,\ldots,l\}, H_i(o) = H_i(q)$$
	 3. Approximately verify the most nearest neighbor by computing the actual distance for all candidates and return the nearest one to the query

### Drawbacks
* Superhash for AND operation is too strong for error tolerance.
* Not adaptive to the distribution of the distances (i.e., requiring manual tuning parameters to be adaptive)

## More Advanced LSH

### Multi-Probe LSH

> [Multi-Probe LSH: Efficient Indexing for High-Dimensional Similarity Search](https://www.cs.princeton.edu/cass/papers/mplsh_vldb07.pdf)
>
> The key idea of the multi-probe LSH method is to use a carefully derived probing sequence to check multiple buckets that are likely to contain the nearest neighbors of a query object. Given the property of locality sensitive hashing, we know that if an object is close to a query object q but not hashed to the same bucket as q, it is likely to be in a bucket that is “close by” (i.e., the hash values of the two buckets only differ slightly). So our goal is to locate these “close by” buckets, thus increasing the chance of finding the objects that are close to q.

* Without multi-probe, for each of $k$ hashes, there are 2 neighbors adjacent to it, which makes three hashes to be considered with. In total there are $3^k$ many hashes to compute.
* Pros
  * Requires less $l$
  * More robust against the unlucky points
* Cons
  * Lose the theoretical guarantee about the results
  * Not parallel-friendly (unite more work on the single worker)

### Collision Counting LSH (C2LSH)

1. Counting the Collisions
	* Collisions are the matches on a single hash function
		> Hash function has been changed from $h_{i,j}(\cdot)$ in LSH function to $h_i(\cdot)$ in C2LSH
	* Use the number of collisions to determine the candidates
		> The more number of collisions there is, the more possibly that they have the same hashing key
	* Match one of the super hash with query $q$ collides at least $\alpha m$ hash values with query $q$.
		> $m$ is the number of hash functions, $\alpha < 0$
# MapReduce

## More Detailed MapReduce Dataflow

* The number of **reducers** can be not only single one but also
  multiple. When it happens, the map tasks partition their output:

    1. One partition for each reduce task as the output from map tasks
     are partitioned into one large key-value pairs and are merged
     altogether.
    2. For each key-value pairs output from map tasks, the key-value
       pairs with the same key have to be partitioned into the same
       partition.

       > For example, we have three pairs output</br>
       > p1-<k1, v1>, p2-<k1, v2>, p3-<k2, v3></br>
       > p1 and p2 have to be partitioned into the same partition
       > while p3 does not have to. Otherwise, in the different
       > reducers, v1 and v2 will not be able to do the arithmetic
       > operation.

    3. Partitioning can be controlled by a user-defined partitioning
       function.
    
## Shuffle

**Shuffle is the process of data redistribution where,**

* Guarantee each reducer obtains all values associated with the same
  key (mentioned above).
* Essentiall process for all of the operations requiring grouping
  (e.g., word count, compute avg)

### Shuffle in Hadoop

* Happens between each **Map** and **Reduce** phase
* Use Shuffle and Sort mechanism (output of each Mapper are sorted by
  the key)
* Use combiner to reduce the amount of data shuffled

    * Combiner combines key-value pairs with the same key in each
      partition (reduce by key)
    * Might need to combine the extra value (e.g., total amount for
      average)
    * HAVE TO BE DONE BY USER

### Shuffle in Spark

* Triggere by certain operations

    * Distinct, Join, Repartition, all *By, *ByKey
    * Hash/Sort shuffle
    * [Exclude] [Tungsten
      shuffle-sort](https://issues.apache.org/jira/browse/SPARK-7081)

#### Hash Shuffle

* Data are initially hash partitioned on the **Map** side
* Then there are total $F$ many files are created to store the
  partitioned data portion
  
  $$ \text{F (\# of files created)} = \text{\# of mappers} \times
  \text{\# of reducers} $$
  
* Use *Consolidate Files* to reduce the number of files created

  from

   $$ M \times R $$ 
   
   to
   
   $$ \text{\# Executor} \times \frac{\# Core}{\# CPU} \times \text{\#
   Reducer} $$
   
* Fast - no memory overhead of sorting
* Space Occupied - large amount of output files

#### Sorting Shuffle

* For each mapper, there are two files created:
    1. Ordered date by key
    2. Index of begining and ending of each chunk
    
    ```
    h1 -> w1
            w1
            w1
            w1
    t1 -> w1
    h2 -> w2
            w2
            .
            w2
    t2 -> w2
    t3 -> .
    ```
* Merged on the fly while being read by reducers
* Default shuffle strategy but fallback to hash shuffle if #
  partitions is small
* Space efficient - smaller amount of files created
* Slow - sorting is slower (O(nlogn)) than hashing (O(n))

## MapReduce Functions in Spark

**The job is a list of Transformations followed by one Action** - only
one action will trigger the 'real' execution - lazy evaluation.

### CombineByKey

**Generic function to combine the elements for each key using a custom
set of aggregation functions.**

```
combineByKey(createCombiner, mergeValue, mergeCombiners,
             numPartitions=None, 
             partitionFunc=<function portable_hash>)
```

Takes three functions:

* `createCombiner`, which turns a V in to C (e.g., creates a
  one-element list)
* `mergeValue`, to merge a V (still V not C yet) into C (e.g., adds it
  to the end of a list)
* `mergeCombiners`, to combine two C's into a single one (e.g., merges
  the lists)

[Visual Explanation](https://stackoverflow.com/questions/29246756/how-createcombiner-mergevalue-mergecombiner-works-in-combinebykey-in-spark-us)

### ReduceByKey

**Merge the values for each key using an assoicative and commutative
reduce function.**

```
reduceByKey(func, numPartitions=None, 
            partitionFunc=<function portable_hash>)
```

### groupByKey

**Group the values for each key in the RDD into a single sequence.
Hash-partitioning the resulting RDD with numPartitions partitions.**

```
groupByKey(numPartitions=None,
           partitionFunc=<function portable_hash>)
```

> The difference between **ReduceByKey** and **GroupByKey** is that
> **ReduceByKey** combines before shuffling. Thus, **ReduceByKey**
> significantly reduce the number of computing.
>
> AVOID using **GroupByKey**

## Efficiency of MapReduce in Spark

* Number of transformations: each transformation involves a linearly
  scan of the dataset (RDD)
  * Optimization: transform RDD less times rather than multiple times in an naive way.
    ```python
    # Bad Deisgn
    maxByKey = rdd.combineByKey()
    sumByKey = rdd.combineByKey()
    sumMaxRdd = maxByKey.join(sumByKey)
    
    # Good Design
    sumMaxRdd = rdd.combineByKey() 
    ```
* Size of transformations: the smaller the input size is, the less
  linearly scan costs
  * Optimization: filter the input then transform would reduce the
    number of operations.
    ```python
    # Bad Design
    countRdd = rdd.reduceByKey()
    filteredRDD = countRdd.filter()
    
    # Good Design
    filteredRDD = countRdd.filter()
    countRdd = rdd.reduceByKey()
    ```
* Shuffles: data transferring between partitions is costly
  * Optimization: partition the data by some sort of characteristics
  required in the task and then reduce
    ```python
    # Bad Design
    countRdd = rdd.reduceByKey()
    countBy2ndCharRdd = countRdd.map().reduceByKey()
    
    # Good Design
    partitionRdd = data.partitionBy()
    countBy2ndCharRdd = patitionRdd.map().reduceByKey()
    ```

## Merge Two RDDs

1. Union: concatenate two RDDs
2. Zip: pair two RDDs into key-value pairs (i.e., the first RDD
   becomes the key and the next RDD becomes the value)
3. Join: merge based on the keys from 2 RDDs

### Union

Inverse operations includes `filter(), substract()`

* Case 1: Different partitioner (None vs. None also counted)

    ```python
    A: contains partitioner A1 and A2
    B: contains partitioner B1 and B2
    
    def union_1(A, B):
        # there will be of size of len(A) + len(B)
        return A.tolist().extend(B.tolist())
    ```
    
* Case 2: Same partitioner

    ```python
    A: contains partitioner A1 and A2
    B: contains partitioner B1 and B2
    
    def union_2(A, B):
        # there will be of size of len(A) or len(B)
        return list(map(list, zip(A, B)))
    ```

### Zip

Inverse operations includes `map(key), map(value)`

**Presuming RDD A and RDD B have the same number of partitions and the same
number elements in each partition.**

* Key-Value pairs after `A.zip(B)`

    ```
    A: [(k1, v1), (k2,v2)]
    B: [(k'1, v'1), (k'2, v'2)]
    
    Output: [((k1,v1), (k'1,v'1)), ((k2,v2), (k'2,v'2))]
    ```

### Join

Inverse operations includes `split()`

* `join` - All pairs with **matching** Keys from A and B
  ```
  A: [(a, 1), (b ,2)]
  B: [(b, 3), (c, 4)]
  
  Output: [(b, (2, 3))]
  ```
* `leftOuterJoin`
  * Case 1: in both A and B - all elements 
  * Case 2: in A but not B
  * Case 3: in B but not A
* `rightOuterJoin` - Opposite to `leftOuterJoin`
* `fullOuterJoin` - Union of `leftOuterJoin` and `rightOuterJoin`

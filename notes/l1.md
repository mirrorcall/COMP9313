# Introduction to Big Data Management

> Quoting by Amazon
>
> Big data can be described in terms of data management challenges that - due to increasing volume, velocity and variety of data - cannot be solved with traditional databases.

## Characteristics - 7V's

### I Volume (i.e., Scale. Fundamental)
* Quantity of data being created from all sources
* Challenge
    * Even linear/sublinear algorithms cost a lot when N is very large

### II Variety (i.e., Diversity. Same Entity Different Data)
* Including
    * Different Types: RDBMS/Text/JSON & XML/Graph/Media
    * Different sources: Reviews from IMDB/Rotten Tomatos
* However, they are supposed to be linked altogether
    * E.g., A customer has per se,
        * Social Media - facebook/Inst/Twitter
        * Gaming - Steam/XBox/Play Station
        * Entertain - Netflix/Stan
        * Finance - VISA/Master Card
        * Purchase - ebay/amazon
 * Challenge
     * Data Integration
         * Heterogeneous
            > Heterogeneous data are any data with high variability of data types and formats. They are possibly ambiguous and low quality due to missing values, high data redundancy, and untruthfulness.
            >
            > That can be solved by traditional database with schema mapping, while the difficulty and time complexity is related to the level of heterogenity.
         * Record linkage in variety data
            > Requires identify if two records refer to the same entity.
      * Data Curation
          > Organization and integration of data collected from various sources.
          >
          > Long tail of data variety.

> Long Tail of Data Variety and Data Curation
>
> That is saying that with data transforming from centrailized to decentralized (i.e., the number of data sources, entities and attributes increasing) the less frequent of use of data is supposed to have.

### III Velocity (i.e., Speed.)
* Data is being generated really fast. Accordingly, storage/process/analysis need to be fast.
* Challenge
    * Users: 16 million 1995 -> 3.4 billion 2016
    * IoT (Internet of Things): sensor/surveillance cameras/CCTV
    * Cloud Computing: 26.4 billion 2012 -> 260.5 billion 2020
    * Website: 156 million 2008 -> 1.5 billion 2019
    * Scientific Data
* Category of Processing
    * Batch processing: Feed data in chunks
    * Online (real time) processing: Feed data in real time
* Problem
    * Transferring data
    * Balancing latency/bandwidth and cost

### IV Veracity (i.e., Quality.)
* Dirty data routinely lead to misleading financial reports, strategic business planning decision.
* Uncertainties
    * Statistical biases
    * Lack of data lineage
    * Software bugs
    * Noise
    * Abnormalities
    * Information Security
    * Untrustworthy data sources
    * Falsification
    * Uncertainty and ambiguity of data
    * Duplication of data
    * Out of date and obsolete data
    * Human error
* Challenge
    * Easy to occur
    * Huge effect to downstream applications
    * Difficult to control - identify/handle errors

### V Variability (Same Data Different Meaning)
* Requires a deeper understanding of the data (make use of the context)

### VI Visibility (i.e., Visualization)
* Use common general types of data visualization to present the characteristics of data
    * Charts
    * Tables
    * Graphs
    * Maps
    * Infographics
    * Dashboards
* Challenge
    * Choose the most suitable way to present data
        * Characteristics of data / Purpose of presentation
    * Difficulty of data visualization
        * High dimensional data / Unstrcutured data / Scalability / Dynamics
### VII Value
* Big data should provide value toward some meaningful goal

## Big Data Applications
1. Big Data in Retail
2. Entertainment
3. National Security
4. Science
5. Healthcare

## Big Data Management
* Big data management contains:
    * Acquisition
    * Storage
    * Preparation
    * Visualization
* Big data analytics contains:
    * Analytics
    * Prediction
    * Decision Making
* Intersection for both - Index Construction

### Data Acquisition
* Characteristics: Application Oriented/Comprehensive
* Acquire Data from different sources with different types with different velocities
    * [structured] RDBMS with SQL
    * [unstructured] or structured Text files/Spreadsheets with scripting languages
    * [semi-structured] Website (XML or JSON/Image) with socket/REST/crawler
    * [s/semi/unstructured] scientific data with specially designed software
    * graph data is hard to handle bc of graph isomophism

### Data Storage
* Traditional way [RDBMS] designed for structured data, disk-oriented
* Big data way
    * RDBMS
    * NoSQL
    * Distributed File Systems

### Data Processing
1. Data Preparation
2. Data Exploration (get to know them)
    * Trends/Correlation/Outliers/Statistics
3. Data Pre-processing (cleanse [veracity] and integrate [variety] them)
    * Cleanse Dirty Data (missing/invalid/inconsistent/duplicate/outliers)
    * Merge Data from multiple, complex and heterogeneous resources (unified view)

### Data Curation
Includes all the processes needed for principled and controlled data creation, maintenance, and management, together with the capacity to add value to data

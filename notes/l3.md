# Spark and RDD

## Problems need to be solved

* Data reliability
* Equal split of dat
* Delay of worker
* Failure of worker
* Aggregation the result

> in parallel and distributed processing

## MapReduce on Hadoop

MapReduce is a programming framwork that

* allows perform distributed and parallel processing on large data sets in a distributed environment
* no need to bother about the issues like reliability, fault tolerance
* offers flexibility to write code logic without caring about design issues of system

### Map

* Reads a block of data
* Produces key-value pairs as intermediate outputs

### Reduce

* Receive key-value pairs from multiple map jobs
* Aggregate the intermediate data tuples to the final output

### Advantages

* Parallel processing
    * Jobs are divided to multipe nodes
    * Nodes work simultaneously
    * Processing time reduced
* Data locality
    * Moving processing to the data (faster processing)

### Limitations

* In a aspect of general programming model
    * More suitable for one-pass computation on a **large** dataset
    * Hard to compose and nest multiple operations
    * No ways to expressing iterative operations
* In a aspect of implemented in Hadoop
    * All datasets are read from disk, then stored back onto disk (I/O consumption or block)
    * All data is usually triple-replicated for reliability (storage overhead and rare access)

## Spark

Build on top of hadoop extends the MapReduce model to efficiently using more types of computations (i.e., efficient primitives for data sharing that Hadoop lacking of)

### Features

* Polyglot
* Speed
* Multiple formats
* Lazy evaluation
* Real time computation
* Hadoop integration
* Machine learning

### Architecture

* Master node that takes care of the job execution within the cluster
* Cluster Manager that allocates resourfes across applications
* Worker node that executes the tasks

![](sparkarch.png)

## Resilient Distributed Dataset (RDD)

RDD is the fundamental data structure of Apache Spark. It's essentailly a collection of elements that can be operated in parallel as well as being faulty tolerant

### Features

* In memory computation (fast with cache)
* Partitioning (efficiency consider Reduce vs ReduceByKey)
* Faulty tolerance
* Immutability (data curation)
* Persistence (data curation)
* Coarse-grained operations
* Location-stickiness (data locality)

### RDD Operations

1. **Transformations** is a function that produces new RDD from the
   existing RDDs.
   
   * Transformations are lazy *evaluation*
   
   > It takes RDD as input and produces one or more RDD as output.
   > Each time it creates new RDD when we apply any transformation.
   > Thus the input RDDs cannot be changed since RDD are immutable in
   > nature.
   >
   > Applying transformation built an RDD lineage, with the entire
   > parent RDDs of the finall RDDs. DAG
   
   1. Narrow transformation - In Narrow transformation, all the
      elements taht are required to compute the records in single
      partition live in the single partition of parent RDD. A limited
      subset of partition is used to calculate the result. (E.g.,
      `map(), flatMap(), filter(), sample(), union()`)
   2. Wide transformation - In wide transformation, all the elements
      that are required to compute the records in the single partition
      may live in many partitions of parent RDD. The partition may
      live in many partitions of parent RDD. (E.g., `groupbyKey(),
      reducebyKey(), sortByKey(), join()`)
      
      > `join()` is a special operator that
      > * Is narrow transformation if join with inputs co-partitionedco
      > * Is wide transformation if join with inputs not co-partitioned
   
2. **Actions** is performed when working with the actual dataset.
    * collect()
    * take()
    * reduce()
    * forEach()
    * count()
    * save()

## Lineage

RDD lineage is the graph of all the ancestor RDDs of an RDD (AKA. RDD operator graph or RDD dependency graph) containing **Nodes** (RDDS) and **Edges** (dependencies between RDDs). It is built as a consequence of applying transformations to the RDD and creates a logical execution plan.

![](sparklineage.png)

### Faulty tolerance

All the RDDs generated from faluty tolerant data are **Fault Tolerant**. If a worker fails (any partition of an RDD lost) then

* Partition can be re-computed from the original dataset using the lineage
* Task will be assigned to another worker

## DAG

DAG is a direct acyclic graph containing **Nodes** (RDDs, results) and **Edges** (Operations to be applied on RDD directs from earlier to later). On the calling of Action, the created DAG submits to DAG Scheduler which further splits the graph into the stages of the task (i.e., explains why Action costs).

DAG scheduler splits the graph into multiple stages. Stages are splited based on transformations,

* The narrow transformations will be grouped together into a single stage
* The wide transformation define the boundary of 2 stages
* The stages needs to be splitted backwards (i.e., from back to upfront)

## Lineage vs. DAG in Spark

* They are both DAG
* Different end nodes
* Different rules in Spark
